{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Network (DQN)\n",
    "\n",
    "Reference Paper: [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/abs/1312.5602)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience\n",
    "\n",
    "time step $t$에서 $t+1$로 transition 발생 시 관찰된 경험:\n",
    "\n",
    "* `obs`: `(num_envs, *obs_shape)`\n",
    "* `action`: `(num_envs, 1)`\n",
    "* `next_obs`: `(num_envs, *obs_shape)`\n",
    "* `reward`: `(num_envs, 1)`\n",
    "* `terminated`: `(num_envs, 1)`\n",
    "\n",
    "`obs`는 observation으로 state와 대응되는 개념임. 다만 일반적인 환경에서 state에 대한 모든 정보가 아닌 관찰된 일부분만 얻기 때문에 보통 observation이라고 명칭함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import numpy as np\n",
    "\n",
    "class Experience(NamedTuple):\n",
    "    obs: np.ndarray\n",
    "    action: np.ndarray\n",
    "    next_obs: np.ndarray\n",
    "    reward: np.ndarray\n",
    "    terminated: np.ndarray"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay Buffer\n",
    "\n",
    "경험을 replay buffer에 저장해 놓았다가 training 시 경험을 랜덤하게 샘플링한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "import operator\n",
    "import torch\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, \n",
    "                 training_freq: int,\n",
    "                 sampled_batch_size: int,\n",
    "                 capacity: int,\n",
    "                 num_envs: int) -> None:\n",
    "        self.training_freq = training_freq\n",
    "        self.sampled_batch_size = sampled_batch_size\n",
    "        self.capacity = capacity\n",
    "        self.num_envs = num_envs\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset experience replay, which is to clear memories.\"\"\"\n",
    "        self.n_step = 0\n",
    "        self.count = 0\n",
    "        self.recent_idx = -1\n",
    "        \n",
    "        self.obs = [None] * self.capacity\n",
    "        self.action = [None] * self.capacity\n",
    "        self.next_obs = [None] * self.capacity\n",
    "        self.reward = [None] * self.capacity\n",
    "        self.terminated = [None] * self.capacity\n",
    "        \n",
    "    @property\n",
    "    def can_sample(self) -> bool:\n",
    "        \"\"\"Is able to sample from experience replay.\"\"\"\n",
    "        return self.count >= self.sampled_batch_size and self.n_step >= self.training_freq\n",
    "        \n",
    "    def add(self, experience: Experience):\n",
    "        \"\"\"Add an experience.\"\"\"\n",
    "        num_envs = experience.obs.shape[0]\n",
    "        self.n_step += 1\n",
    "        \n",
    "        for i in range(self.num_envs):\n",
    "            self.recent_idx = (self.recent_idx + 1) % self.capacity\n",
    "            self.count = min(self.count + 1, self.capacity)\n",
    "            \n",
    "            self.obs[self.recent_idx] = experience.obs[i]\n",
    "            self.action[self.recent_idx] = experience.action[i]\n",
    "            self.next_obs[self.recent_idx] = experience.next_obs[i]\n",
    "            self.reward[self.recent_idx] = experience.reward[i]\n",
    "            self.terminated[self.recent_idx] = experience.terminated[i]\n",
    "        \n",
    "    def sample(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Samples experience batch from it. Default sampling distribution is uniform.\n",
    "\n",
    "        Returns:\n",
    "            obs, action, next_obs, reward, terminated\n",
    "        \"\"\"\n",
    "        self.n_step = 0\n",
    "        # 경험 인덱스 샘플링\n",
    "        sample_idx = self._sample_idxs()\n",
    "        \n",
    "        get_batch_tensor = lambda x: self._get_batch_tensor(x, sample_idx)\n",
    "        \n",
    "        obs = get_batch_tensor(self.obs)\n",
    "        action = get_batch_tensor(self.action)\n",
    "        next_obs = get_batch_tensor(self.next_obs)\n",
    "        reward = get_batch_tensor(self.reward)\n",
    "        terminated = get_batch_tensor(self.terminated)\n",
    "        \n",
    "        return obs, action, next_obs, reward, terminated\n",
    "    \n",
    "    def _sample_idxs(self) -> np.ndarray:\n",
    "        \"\"\"인덱스 랜덤 샘플링.\"\"\"\n",
    "        batch_idxs = np.random.randint(self.count, size=self.sampled_batch_size)\n",
    "        return batch_idxs\n",
    "    \n",
    "    def _get_batch_tensor(self, items: list, batch_idx: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"`batch_idx`에 따라 리스트 item을 가져옴.\"\"\"\n",
    "        return np.array(operator.itemgetter(*batch_idx)(items))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Configuration\n",
    "\n",
    "* `training_freq`: 훈련 빈도\n",
    "* `batch_size`: replay buffer로부터 sampling할 경험 개수\n",
    "* `capacity`: replay buffer에 저장되는 최대 경험 개수\n",
    "* `epoch`: 각 훈련 빈도마다 파라미터 업데이트 횟수\n",
    "* `epsilon_linear_decay`: `(start_t, end_t, start_value, end_value)`\n",
    "* `gamma`: discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNConfig(NamedTuple):\n",
    "    training_freq: int\n",
    "    batch_size: int\n",
    "    capacity: int\n",
    "    epoch: int\n",
    "    epsilon_linear_decay: Tuple[int, int, float, float]\n",
    "    gamma: float = 0.99"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Greedy Policy\n",
    "\n",
    "Q value로부터 epsilon greedy distribution을 획득한다.  \n",
    "DQN과 같은 가치 기반 방법에서 사용하며 discrete action에만 사용 가능하다.  \n",
    "여기서는 discrete action branch가 1이라고 가정한다. \n",
    "\n",
    "`epsilon`: 0 ~ 1 사이의 값으로 0에 가까울 수록 exploitation, 1에 가까울 수록 exploration을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Categorical\n",
    "\n",
    "class EpsilonGreedyPolicy:\n",
    "    def __init__(self, epsilon: float) -> None:\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    def sample(self, q_values: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Q value로부터 action을 샘플링한다.\n",
    "\n",
    "        Args:\n",
    "            q_values (torch.Tensor): `(num_envs, num_actions)`의 shape\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: selected action `(num_envs, 1)`\n",
    "        \"\"\"\n",
    "        num_actions = q_values.shape[1]\n",
    "        \n",
    "        # epsilon-greedy probabilities\n",
    "        greedy_action_prob  = 1.0 - self.epsilon + self.epsilon / num_actions\n",
    "        non_greedy_action_prob = self.epsilon / num_actions\n",
    "        \n",
    "        # get greedy action\n",
    "        greedy_action = q_values.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        # set epsilon greedy probability distribution\n",
    "        epsilon_greedy_prob = torch.full_like(q_values, non_greedy_action_prob)\n",
    "        epsilon_greedy_prob.scatter_(1, greedy_action, greedy_action_prob)\n",
    "        epsilon_greedy_dist = Categorical(probs=epsilon_greedy_prob)\n",
    "        return epsilon_greedy_dist.sample().unsqueeze_(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self,\n",
    "                 config: DQNConfig,\n",
    "                 network: nn.Module,\n",
    "                 optimizer: optim.Optimizer,\n",
    "                 num_envs: int = 1) -> None:\n",
    "\n",
    "        self.config = config\n",
    "        self.network = network\n",
    "        self.optimizer = optimizer\n",
    "        self.policy = EpsilonGreedyPolicy(self.config.epsilon_linear_decay[2])\n",
    "        self.num_envs = num_envs\n",
    "        \n",
    "        self.replay_buffer = ReplayBuffer(\n",
    "            self.config.training_freq,\n",
    "            self.config.batch_size,\n",
    "            self.config.capacity,\n",
    "            self.num_envs\n",
    "        )\n",
    "        \n",
    "        self.global_time_step = 0\n",
    "        self.training_step = 0\n",
    "        \n",
    "        self.num_losses = 0\n",
    "        self.avg_loss = 0.0\n",
    "        \n",
    "    def select_action(self, obs: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        현재 state에서 action을 선택한다.\n",
    "\n",
    "        Args:\n",
    "            obs (np.ndarray): `(num_envs, *obs_shape)`\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: `(num_envs, 1)`\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # feed forward\n",
    "            q_values = self.network(torch.from_numpy(obs))\n",
    "            \n",
    "            # action sampling\n",
    "            action = self.policy.sample(q_values)\n",
    "        \n",
    "            return action.detach().cpu().numpy()\n",
    "        \n",
    "    def update(self, experience: Experience):\n",
    "        self.global_time_step += 1\n",
    "        self.replay_buffer.add(experience)\n",
    "        \n",
    "        if self.replay_buffer.can_sample:\n",
    "            self.train()\n",
    "            self._epsilon_decay(self.global_time_step)\n",
    "            \n",
    "    def train(self):\n",
    "        for _ in range(self.config.epoch):\n",
    "            # 경험 샘플링\n",
    "            exp_batch = self.replay_buffer.sample()\n",
    "            obs, action, next_obs, reward, terminated = self._to_tensor(exp_batch)\n",
    "            \n",
    "            # loss 계산\n",
    "            td_loss = self.compute_td_loss(obs, action, next_obs, reward, terminated)\n",
    "            \n",
    "            # gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            td_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # log info\n",
    "            self.training_step += 1\n",
    "            self.num_losses += 1\n",
    "            # incremental mean\n",
    "            self.avg_loss += 1.0 / self.num_losses * (td_loss.item() - self.avg_loss)\n",
    "            \n",
    "    def compute_td_loss(self, \n",
    "                        obs: torch.Tensor, \n",
    "                        action: torch.Tensor,\n",
    "                        next_obs: torch.Tensor,\n",
    "                        reward: torch.Tensor,\n",
    "                        terminated: torch.Tensor) -> torch.Tensor:\n",
    "        # 현재 state에서의 모든 action에 대한 Q value 추정\n",
    "        q_values: torch.Tensor = self.network(obs)\n",
    "        with torch.no_grad():\n",
    "            # 다음 state에서의 모든 action에 대한 Q value 추정\n",
    "            next_q_values = self.network(next_obs)\n",
    "        \n",
    "        # 현재 state에서 선택된 action에 대한 Q value\n",
    "        q_value = q_values.gather(dim=1, index=action)\n",
    "        # next state에서의 최대 Q value\n",
    "        next_max_q_value, _ = next_q_values.max(dim=1, keepdim=True)\n",
    "        # Q target 계산\n",
    "        not_terminated = 1 - terminated\n",
    "        q_target = reward + not_terminated * self.config.gamma * next_max_q_value\n",
    "        \n",
    "        # TD loss 계산\n",
    "        td_loss = F.mse_loss(q_target, q_value)\n",
    "        \n",
    "        return td_loss\n",
    "            \n",
    "    def _to_tensor(self, exp_batch):\n",
    "        exp_batch_tensor = []\n",
    "        for item in exp_batch:\n",
    "            exp_batch_tensor.append(torch.from_numpy(item))\n",
    "        return tuple(exp_batch_tensor)\n",
    "    \n",
    "    def _epsilon_decay(self, t: float):\n",
    "        start_t, end_t, start_value, end_value = self.config.epsilon_linear_decay\n",
    "        slope = (end_value - start_value) / (end_t - start_t)\n",
    "        value = np.clip(slope * (t - start_t) + start_value, end_value, start_value)\n",
    "        self.policy.epsilon = value\n",
    "        \n",
    "    @property\n",
    "    def log_data(self) -> dict:\n",
    "        ld = {\"Epsilon\": (self.policy.epsilon, self.global_time_step)}\n",
    "        if self.num_losses > 0:\n",
    "            ld[\"TD Loss\"] = (self.avg_loss, self.training_step)\n",
    "            self.num_losses = 0\n",
    "            self.avg_loss = 0.0\n",
    "        return ld"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망\n",
    "\n",
    "아래와 같이 구성됨:\n",
    "\n",
    "layer 1: 입력층  \n",
    "layer 2: 은닉층  \n",
    "layer 3: 출력층 - $Q(S_t, \\cdot)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQNet(nn.Module):\n",
    "    def __init__(self, num_obs_features: int, num_actions: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_obs_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(obs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole 환경 생성\n",
    "\n",
    "`num_envs`는 `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "gym_env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_shape = gym_env.observation_space.shape\n",
    "obs_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions = gym_env.action_space.n\n",
    "num_actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Agent 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DQNConfig(\n",
    "    training_freq=16,\n",
    "    batch_size=128,\n",
    "    capacity=1000,\n",
    "    epoch=3,\n",
    "    epsilon_linear_decay=(0, 100000, 0.3, 0.01),\n",
    "    gamma=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = CartPoleQNet(obs_shape[0], num_actions)\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQN(\n",
    "    config,\n",
    "    network,\n",
    "    optimizer\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, average reward: 12.00\n",
      "episode: 10, average reward: 12.80\n",
      "episode: 20, average reward: 11.60\n",
      "episode: 30, average reward: 10.40\n",
      "episode: 40, average reward: 10.70\n",
      "episode: 50, average reward: 12.90\n",
      "episode: 60, average reward: 10.80\n",
      "episode: 70, average reward: 11.70\n",
      "episode: 80, average reward: 18.40\n",
      "episode: 90, average reward: 14.90\n",
      "episode: 100, average reward: 23.60\n",
      "episode: 110, average reward: 30.20\n",
      "episode: 120, average reward: 46.90\n",
      "episode: 130, average reward: 84.80\n",
      "episode: 140, average reward: 120.20\n",
      "episode: 150, average reward: 91.80\n",
      "episode: 160, average reward: 51.20\n",
      "episode: 170, average reward: 69.00\n",
      "episode: 180, average reward: 349.20\n",
      "episode: 190, average reward: 270.90\n",
      "episode: 200, average reward: 112.10\n",
      "episode: 210, average reward: 174.10\n",
      "episode: 220, average reward: 360.30\n",
      "episode: 230, average reward: 419.10\n",
      "episode: 240, average reward: 359.70\n",
      "episode: 250, average reward: 453.60\n",
      "episode: 260, average reward: 429.60\n",
      "episode: 270, average reward: 500.00\n",
      "episode: 280, average reward: 375.50\n",
      "episode: 290, average reward: 12.60\n",
      "episode: 300, average reward: 59.60\n",
      "episode: 310, average reward: 323.00\n",
      "episode: 320, average reward: 457.30\n",
      "episode: 330, average reward: 500.00\n",
      "episode: 340, average reward: 475.90\n",
      "episode: 350, average reward: 460.00\n",
      "episode: 360, average reward: 160.70\n",
      "episode: 370, average reward: 162.70\n",
      "episode: 380, average reward: 481.80\n",
      "episode: 390, average reward: 441.30\n",
      "episode: 400, average reward: 349.40\n",
      "episode: 410, average reward: 358.70\n",
      "episode: 420, average reward: 389.90\n",
      "episode: 430, average reward: 386.20\n",
      "episode: 440, average reward: 359.60\n",
      "episode: 450, average reward: 390.50\n",
      "episode: 460, average reward: 406.90\n",
      "episode: 470, average reward: 418.60\n",
      "episode: 480, average reward: 423.30\n",
      "episode: 490, average reward: 452.10\n",
      "episode: 500, average reward: 470.10\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = SummaryWriter(\"results/CartPole-v1_DQN\")\n",
    "\n",
    "num_episodes = 501\n",
    "summary_freq = 10\n",
    "\n",
    "cumulative_rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    cumulative_reward = 0.0\n",
    "    \n",
    "    obs, _ = gym_env.reset()\n",
    "    obs = obs[np.newaxis, ...].astype(np.float32) # (*obs_shape) -> (1, *obs_shape)\n",
    "    terminated = False\n",
    "    \n",
    "    while not terminated:\n",
    "        # take action and observe\n",
    "        action = dqn.select_action(obs)\n",
    "        next_obs, reward, terminated, truncated, _ = gym_env.step(action.item())\n",
    "        terminated = terminated | truncated\n",
    "        \n",
    "        # DQN 업데이트\n",
    "        experience = Experience(\n",
    "            obs,\n",
    "            action,\n",
    "            next_obs[np.newaxis, ...].astype(np.float32),\n",
    "            np.array([[reward]]).astype(np.float32),\n",
    "            np.array([[terminated]], dtype=np.float32).astype(np.float32)\n",
    "        )\n",
    "        dqn.update(experience)\n",
    "        \n",
    "        # reward 누적\n",
    "        cumulative_reward += reward\n",
    "        \n",
    "        # 현재 observation 업데이트\n",
    "        obs = experience.next_obs\n",
    "        \n",
    "    cumulative_rewards.append(cumulative_reward)\n",
    "    \n",
    "    if episode % summary_freq == 0:\n",
    "        avg_reward = np.mean(cumulative_rewards)\n",
    "        cumulative_rewards.clear()\n",
    "        print(f\"episode: {episode}, average reward: {avg_reward:.2f}\")\n",
    "        logger.add_scalar(\"Cumulative Reward\", avg_reward, episode)\n",
    "        for key, value in dqn.log_data.items():\n",
    "            logger.add_scalar(key, value[0], value[1])\n",
    "            \n",
    "logger.flush()\n",
    "logger.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 커맨드를 anaconda shell에 입력해 결과 확인:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=results\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-expert-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e1ca3a1c04257685a44c086005140006859e2cf07e0d291a94ac6e18fe0a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
