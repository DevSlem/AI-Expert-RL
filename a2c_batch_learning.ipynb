{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic (A2C) - Batch Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Advantage Estimation (GAE)\n",
    "\n",
    "Reference: [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)\n",
    "\n",
    "Advantage Function:\n",
    "\n",
    "$$\n",
    "A^\\pi(s_t, a_t) := Q^\\pi(s_t, a_t) - V^\\pi(s_t)\n",
    "$$\n",
    "\n",
    "TD error $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$를 정의하자. $n$-step Advantage 추정치를 아래와 같이 정의할 수 있다:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{A}_t^{(1)} &:= \\delta_t^V &= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "    \\hat{A}_t^{(2)} &:= \\delta_t^V + \\gamma \\delta_{t+1}^V &= r_t + \\gamma r_{t+1} + \\gamma^2 V(s_{t+2}) - V(s_t) \\\\\n",
    "    \\hat{A}_t^{(3)} &:= \\delta_t^V + \\gamma \\delta_{t+1}^V + \\gamma^2 \\delta_{t+2}^V &= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\gamma^3 V(s_{t+3}) - V(s_t) \\\\\n",
    "    \\\\\n",
    "    \\hat{A}_t^{(n)} &:= \\sum_{l=0}^{n-1}\\gamma^l \\delta_{t+l}^V &= r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + \\cdots + \\gamma^n V(s_{t+n}) - V(s_t) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "*Generalized Advantage Estimator* (GAE)는 $n$-step Advantage 추정치의 exponentially-weighted average로 정의된다:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{A}_t^{\\text{GAE}(\\gamma, \\lambda)} &:= (1 - \\lambda)\\Big(\\hat{A}_t^{(1)} + \\lambda \\hat{A}_t^{(2)} + \\lambda^2 \\hat{A}_t^{(3)} + \\dots \\Big) \\\\\n",
    "    &= \\sum_{l=0}^\\infty (\\gamma \\lambda)^l \\delta_{t+l}^V\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "만약 $lambda = 0$일 경우 $\\hat{A}_t := \\delta_t$로 one-step TD error이다. 따라서 분산이 낮지만 편향이 크다. 반대로 $\\lambda = 1$일 경우 $\\hat{A}_t := \\sum_{l=0}^\\infty \\gamma^l \\delta_{t+l} = \\sum_{l=0}^\\infty \\gamma^l r_{t+l} - V(s_t)$로 편향은 낮지만 분산이 크다. 따라서 $\\lambda$는 분산과 편향 사이의 tradeoff를 적절히 조정하는 파라미터이다.\n",
    "\n",
    "계산적 트릭을 사용하면 GAE를 recursive하게 계산할 수 있다:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\hat{A}_t^{\\text{GAE}} = \\delta_t^V + (\\gamma \\lambda) \\hat{A}_{t+1}^{\\text{GAE}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_gae(state_value: torch.Tensor, \n",
    "                reward: torch.Tensor, \n",
    "                terminated: torch.Tensor,\n",
    "                gamma: float,\n",
    "                lam: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute generalized advantage estimation (GAE) during n-step transitions. See details in https://arxiv.org/abs/1506.02438.\n",
    "\n",
    "    Args:\n",
    "        state_value (Tensor): `(n_steps + 1,)`, 마지막 transition에서의 next state value를 포함한 n-step 동안의 state value\n",
    "        rewards (Tensor): `(n_steps,)`, n-step 동안의 reward\n",
    "        terminateds (Tensor): `(n_steps,)`, n-step 동안의 terminated\n",
    "        gamma (float): discount factor for future rewards\n",
    "        lam (float): lambda which controls the balanace between bias and variance\n",
    "\n",
    "    Returns:\n",
    "        gae (Tensor): `(n_steps,)`, n-step 동안의 GAE\n",
    "    \"\"\"\n",
    "    \n",
    "    n_step = len(reward)\n",
    "    gae = torch.empty_like(reward)\n",
    "    discounted_gae = 0.0 # GAE at time step t+n\n",
    "    not_terminated = 1 - terminated\n",
    "    delta = reward + not_terminated * gamma * state_value[1:] - state_value[:-1]\n",
    "    discount_factor = gamma * lam\n",
    "    \n",
    "    # compute GAE\n",
    "    for t in reversed(range(n_step)):\n",
    "        discounted_gae = delta[t] + not_terminated[t] * discount_factor * discounted_gae\n",
    "        gae[t] = discounted_gae\n",
    "     \n",
    "    return gae"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A2C Agent in CartPole-v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network\n",
    "\n",
    "Actor와 Critic이 parameter를 공유하는 parameter sharing 기법을 사용한다. 이는 Actor와 Critic이 state space에 대한 공통된 feature를 학습할 수 있고, 계산 효율성이 증가하는 장점이 있다. 또한 batch learning 기법을 활용하기 때문에 sample 효율성이 증가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActorCriticSharedNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 obs_features: int,\n",
    "                 num_actions: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        hidden_features = 64\n",
    "        \n",
    "        # parameter sharing\n",
    "        self.encoding_layer = nn.Sequential(\n",
    "            nn.Linear(obs_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, hidden_features),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.actor = nn.Linear(hidden_features, num_actions)\n",
    "        self.critic = nn.Linear(hidden_features, 1)\n",
    "        \n",
    "    def forward(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Feed forward method.\n",
    "\n",
    "        Args:\n",
    "            obs (torch.Tensor): observation\n",
    "\n",
    "        Returns:\n",
    "            logits, state_value\n",
    "        \"\"\"\n",
    "        encoding = self.encoding_layer(obs)\n",
    "        logits = self.actor(encoding)\n",
    "        state_value = self.critic(encoding)\n",
    "        return logits, state_value.squeeze_(dim=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C 구현\n",
    "\n",
    "advantage를 계산하기 위해 GAE를 사용한다. `A2C.train()` 메소드는 $n$-step 마다 호출된다.\n",
    "\n",
    "configuration:\n",
    "\n",
    "* `gamma`: discount factor $\\gamma$\n",
    "* `lam`: $\\lambda$, GAE의 bias-variance tradeoff를 조정\n",
    "* `value_loss_coef`: actor loss와 critic loss 결합 시 critic loss를 원래 값에 어느 정도 반영할 지 조정하는 multiplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class A2C:\n",
    "    def __init__(self,\n",
    "                 obs_features: int,\n",
    "                 num_actions: int,\n",
    "                 gamma: float = 0.99,\n",
    "                 lam: float = 0.95,\n",
    "                 value_loss_coef: float = 0.5) -> None:\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        \n",
    "        self.actor_critic = ActorCriticSharedNetwork(obs_features, num_actions)\n",
    "        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=0.001)\n",
    "    \n",
    "    def select_action(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns action, log pi(a|s), state value V(s).\n",
    "        \"\"\"\n",
    "        # feed forward\n",
    "        logits, state_value = self.actor_critic(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_pi = dist.log_prob(action)\n",
    "        return action, log_pi, state_value\n",
    "    \n",
    "    def train(self,\n",
    "              fianl_next_obs: torch.Tensor,\n",
    "              reward: torch.Tensor,\n",
    "              terminated: torch.Tensor,\n",
    "              log_pi: torch.Tensor,\n",
    "              state_value: torch.Tensor) -> Tuple[float, float]:\n",
    "        \"\"\"\n",
    "        Train A2C agent.\n",
    "\n",
    "        Args:\n",
    "            fianl_next_obs (torch.Tensor): `(obs_features,)`\n",
    "            reward (torch.Tensor): `(n_steps,)`\n",
    "            terminated (torch.Tensor): `(n_steps,)`\n",
    "            log_pi (torch.Tensor): `(n_steps,)`\n",
    "            state_value (torch.Tensor): `(n_steps,)`\n",
    "\n",
    "        Returns:\n",
    "            actor_loss (float): computed actor loss\n",
    "            critic_loss (float): computed critic loss\n",
    "        \"\"\"\n",
    "        # compute final next state value\n",
    "        with torch.no_grad():\n",
    "            _, final_next_state_value = self.actor_critic(fianl_next_obs)\n",
    "            # (n_steps + 1,)\n",
    "            state_value_with_final = torch.cat((state_value, final_next_state_value.unsqueeze(0)))\n",
    "        \n",
    "        # compute advantage\n",
    "        advantage = compute_gae(\n",
    "            state_value_with_final,\n",
    "            reward,\n",
    "            terminated,\n",
    "            self.gamma,\n",
    "            self.lam\n",
    "        )\n",
    "        \n",
    "        # compute target state value\n",
    "        target_state_value = advantage + state_value.detach()\n",
    "        \n",
    "        # compute actor-critic loss\n",
    "        actor_loss = self._compute_actor_loss(log_pi, advantage)\n",
    "        critic_loss = self._compute_critic_loss(state_value, target_state_value)\n",
    "        \n",
    "        # train step\n",
    "        loss = actor_loss + self.value_loss_coef * critic_loss\n",
    "        self._train_step(loss)\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "        \n",
    "        \n",
    "    def _compute_actor_loss(self,\n",
    "                           log_pi: torch.Tensor,\n",
    "                           advantage: torch.Tensor) -> torch.Tensor:\n",
    "        return -(advantage * log_pi).mean()\n",
    "    \n",
    "    def _compute_critic_loss(self,\n",
    "                            state_value: torch.Tensor,\n",
    "                            target_state_value: torch.Tensor) -> torch.Tensor:\n",
    "        return F.mse_loss(state_value, target_state_value)\n",
    "    \n",
    "    def _train_step(self,\n",
    "                    loss: torch.Tensor):\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make CartPole-v1 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(env_id) # for training\n",
    "inference_env = gym.make(env_id, render_mode=\"human\") # for inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_features = env.observation_space.shape\n",
    "obs_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C(obs_features[0], num_actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0003315 , -0.04382272, -0.04053487, -0.01080584], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0),\n",
       " tensor(-0.7390, grad_fn=<SqueezeBackward1>),\n",
       " tensor(0.0305, grad_fn=<SqueezeBackward3>))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, log_pi, state_value = agent.select_action(torch.from_numpy(obs))\n",
    "(action, log_pi, state_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00054495, -0.2383406 , -0.04075098,  0.26881734], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_buffer(n_steps: int):\n",
    "    reward_arr = [None] * n_steps\n",
    "    terminated_arr = [None] * n_steps\n",
    "    log_pi_arr = [None] * n_steps\n",
    "    state_value_arr = [None] * n_steps\n",
    "    return reward_arr, terminated_arr, log_pi_arr, state_value_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The key you just pressed is not recognized by SDL. To help get this fixed, please report this to the SDL forums/mailing list <https://discourse.libsdl.org/> X11 KeyCode 208 (200), X11 KeySym 0x0 ((null)).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference - episode: 0, cumulative reward: 12.0\n",
      "inference - episode: 50, cumulative reward: 24.0\n",
      "inference - episode: 100, cumulative reward: 14.0\n",
      "inference - episode: 150, cumulative reward: 21.0\n",
      "inference - episode: 200, cumulative reward: 21.0\n",
      "inference - episode: 250, cumulative reward: 77.0\n",
      "inference - episode: 300, cumulative reward: 89.0\n",
      "inference - episode: 350, cumulative reward: 98.0\n",
      "inference - episode: 400, cumulative reward: 228.0\n",
      "inference - episode: 450, cumulative reward: 222.0\n",
      "inference - episode: 500, cumulative reward: 277.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = SummaryWriter(\"results/CartPole-v1_A2C_BatchLearning\")\n",
    "\n",
    "total_episodes = 501\n",
    "inference_freq = 50\n",
    "n_steps = 16\n",
    "\n",
    "# reset buffers\n",
    "t = 0\n",
    "reward_arr, terminated_arr, log_pi_arr, state_value_arr = reset_buffer(n_steps)\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    \n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        # take action and observe\n",
    "        action, log_pi, state_value = agent.select_action(torch.from_numpy(obs))\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "        terminated = terminated | truncated\n",
    "        \n",
    "        # update buffer\n",
    "        reward_arr[t] = reward\n",
    "        terminated_arr[t] = terminated\n",
    "        log_pi_arr[t] = log_pi\n",
    "        state_value_arr[t] = state_value\n",
    "        t += 1\n",
    "        \n",
    "        if t == n_steps:\n",
    "            # train the agent\n",
    "            actor_loss, critic_loss = agent.train(\n",
    "                torch.from_numpy(next_obs),\n",
    "                torch.tensor(reward_arr),\n",
    "                torch.tensor(terminated_arr, dtype=torch.float32),\n",
    "                torch.stack(log_pi_arr),\n",
    "                torch.stack(state_value_arr)\n",
    "            )\n",
    "            \n",
    "            # log losses\n",
    "            logger.add_scalar(\"Actor Loss\", actor_loss, episode)\n",
    "            logger.add_scalar(\"Critic Loss\", critic_loss, episode)\n",
    "            \n",
    "            # reset buffers\n",
    "            t = 0\n",
    "            reward_arr, terminated_arr, log_pi_arr, state_value_arr = reset_buffer(n_steps)\n",
    "                \n",
    "        # next step\n",
    "        cumulative_reward += reward\n",
    "        obs = next_obs\n",
    "        \n",
    "    # log cumulative reward\n",
    "    logger.add_scalar(\"Cumulative Reward\", cumulative_reward, episode)\n",
    "    \n",
    "    # inference\n",
    "    if episode % inference_freq == 0:\n",
    "        obs, _ = inference_env.reset()\n",
    "        terminated = False\n",
    "        inference_cumulative_reward = 0\n",
    "        while not terminated:\n",
    "            with torch.no_grad():\n",
    "                action, _, _ = agent.select_action(torch.from_numpy(obs))\n",
    "            next_obs, reward, terminated, truncated, _ = inference_env.step(action.numpy())\n",
    "            terminated = terminated | truncated\n",
    "\n",
    "            inference_cumulative_reward += reward\n",
    "            obs = next_obs\n",
    "            \n",
    "        print(f\"inference - episode: {episode}, cumulative reward: {inference_cumulative_reward}\")\n",
    "        \n",
    "logger.flush()\n",
    "logger.close()\n",
    "\n",
    "env.close()\n",
    "inference_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-expert-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31ac01b8258adb5227428d8c0910a8335c058d339333bd5076ec56982dd3aaf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
