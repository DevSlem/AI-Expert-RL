{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE - Monte Carlo Policy Gradient\n",
    "\n",
    "REINFORCE 알고리즘은 다음과 같은 수식을 따라 Gradient Ascent를 수행한다:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_{t+1} \\doteq \\mathbf{\\theta}_t + \\alpha G_t \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}_t)\n",
    "$$\n",
    "\n",
    "각 수식 요소는 아래와 같다:\n",
    "\n",
    "* $\\theta$: policy parameter\n",
    "* $\\alpha$: learning rate\n",
    "* $G_t$: time step $t$에서의 return\n",
    "* $\\pi$: policy\n",
    "* $A_t$: time step $t$에서 policy $\\pi$를 따라 선택된 action\n",
    "* $S_t$: time step $t$에서의 state\n",
    "\n",
    "REINFORCE에 baseline 개념을 적용할 수 있다. baseline 적용의 장점은 업데이트의 기대값을 변화시키지는 않으면서 분산을 낮춰 학습 안정성이 증가하도록 기대할 수 있다. 이 때 baseline은 어떤 것도 가능하다.\n",
    "\n",
    "REINFORCE 알고리즘에 baseline을 적용하면 아래와 같다:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\theta}_{t+1} \\doteq \\mathbf{\\theta}_t + \\alpha \\Big(G_t - b(S_t) \\Big) \\nabla \\ln \\pi(A_t \\vert S_t, \\mathbf{\\theta}_t)\n",
    "$$\n",
    "\n",
    "이 실습 코드에서는 baseline으로 한 episode동안의 return들의 평균을 사용하였다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement REINFORCE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Agent\n",
    "\n",
    "* 신경망 구성\n",
    "* action 결정\n",
    "* loss 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\ai-expert-rl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class REINFORCE(nn.Module):\n",
    "    def __init__(self, obs_features: int, \n",
    "                 num_actions: int,\n",
    "                 gamma: float = 0.99) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = gamma # discount factor\n",
    "        \n",
    "        # simple fully connected layer\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(obs_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "    \n",
    "    def select_action(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        agent의 observation으로부터 action을 선택한다.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: selected_action, log_pi\n",
    "        \"\"\"\n",
    "        pdparam = self.layer(obs)\n",
    "        dist = Categorical(logits=pdparam)\n",
    "        action = dist.sample()\n",
    "        log_pi = dist.log_prob(action)\n",
    "        return action, log_pi\n",
    "    \n",
    "    def compute_return(self, \n",
    "                       reward: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Return G를 계산한다. `reward` tensor는 1차원 벡터로 길이는 episode 길이와 동일하다.\n",
    "        \"\"\"\n",
    "        returns = torch.empty_like(reward)\n",
    "        G = 0.0\n",
    "        T = len(reward)\n",
    "        for t in reversed(range(T)):\n",
    "            G = reward[t] + self.gamma * G\n",
    "            returns[t] = G\n",
    "        return returns\n",
    "    \n",
    "    def compute_reinforce_loss(self,\n",
    "                               returns: torch.Tensor,\n",
    "                               log_pi: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        REINFORCE loss를 계산한다.\n",
    "        \"\"\"\n",
    "        # REINFORCE with baseline\n",
    "        returns = returns - returns.mean()\n",
    "        # gradient ascent를 descent로 바꿔주기 위해 앞에 - 부호를 붙인다\n",
    "        return -(returns * log_pi).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make CartPole Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\") # for training\n",
    "inference_env = gym.make(\"CartPole-v1\", render_mode=\"human\") # for inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_features = env.observation_space.shape\n",
    "obs_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "agent = REINFORCE(obs_features[0], num_actions)\n",
    "optimizer = optim.Adam(agent.parameters(), lr=0.001)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00751555, -0.02465002, -0.00998014,  0.04450459], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), tensor(-0.7011, grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, log_pi = agent.select_action(torch.from_numpy(obs))\n",
    "(action, log_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00800855,  0.17061362, -0.00909005, -0.25131038], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference - episode: 0, cumulative reward: 16.0\n",
      "inference - episode: 50, cumulative reward: 22.0\n",
      "inference - episode: 100, cumulative reward: 79.0\n",
      "inference - episode: 150, cumulative reward: 228.0\n",
      "inference - episode: 200, cumulative reward: 189.0\n",
      "inference - episode: 250, cumulative reward: 500.0\n",
      "inference - episode: 300, cumulative reward: 252.0\n",
      "inference - episode: 350, cumulative reward: 500.0\n",
      "inference - episode: 400, cumulative reward: 232.0\n",
      "inference - episode: 450, cumulative reward: 473.0\n",
      "inference - episode: 500, cumulative reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = SummaryWriter(\"results/CartPole-v1_REINFORCE\")\n",
    "\n",
    "total_episodes = 501\n",
    "inference_freq = 50\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    reward_buffer, log_pi_buffer = [], []\n",
    "    \n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        # take action and observe\n",
    "        action, log_pi = agent.select_action(torch.from_numpy(obs))\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "        terminated = terminated | truncated\n",
    "        \n",
    "        # update buffer\n",
    "        reward_buffer.append(reward) # 스칼라\n",
    "        log_pi_buffer.append(log_pi) # 0차원 tensor\n",
    "        \n",
    "        # update info\n",
    "        cumulative_reward += reward\n",
    "        obs = next_obs\n",
    "        \n",
    "    # buffer to tensor\n",
    "    reward_tensor = torch.tensor(reward_buffer)\n",
    "    log_pi_tensor = torch.stack(log_pi_buffer)\n",
    "    reward_buffer.clear()\n",
    "    log_pi_buffer.clear()\n",
    "    \n",
    "    # compute loss\n",
    "    returns = agent.compute_return(reward_tensor)\n",
    "    loss = agent.compute_reinforce_loss(returns, log_pi_tensor)\n",
    "    \n",
    "    # gradient step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # log data\n",
    "    logger.add_scalar(\"Cumulative Reward\", cumulative_reward, episode)\n",
    "    logger.add_scalar(\"Policy Loss\", loss.item(), episode)\n",
    "    \n",
    "    # inference\n",
    "    if episode % inference_freq == 0:\n",
    "        obs, _ = inference_env.reset()\n",
    "        terminated = False\n",
    "        inference_cumulative_reward = 0\n",
    "        while not terminated:\n",
    "            with torch.no_grad():\n",
    "                action, _ = agent.select_action(torch.from_numpy(obs))\n",
    "            next_obs, reward, terminated, truncated, _ = inference_env.step(action.numpy())\n",
    "            terminated = terminated | truncated\n",
    "\n",
    "            inference_cumulative_reward += reward\n",
    "            obs = next_obs\n",
    "            \n",
    "        print(f\"inference - episode: {episode}, cumulative reward: {inference_cumulative_reward}\")\n",
    "\n",
    "logger.flush()\n",
    "logger.close()\n",
    "\n",
    "env.close()\n",
    "inference_env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 커맨드를 anaconda shell에 입력해 결과 확인:\n",
    "\n",
    "```\n",
    "$ tensorboard --logdir=results\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-expert-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9e1ca3a1c04257685a44c086005140006859e2cf07e0d291a94ac6e18fe0a9c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
