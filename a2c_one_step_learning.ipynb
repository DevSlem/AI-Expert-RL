{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advantage Actor Critic (A2C) - One Step Learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training A2C Agent in CartPole-v1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor-Critic Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor와 Critic 각각이 독립된 구조이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, \n",
    "                 obs_features: int,\n",
    "                 num_actions: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.actor(obs)\n",
    "    \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self,\n",
    "                 obs_features: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(obs_features, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.critic(obs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2C 구현\n",
    "\n",
    "one-step advantage 추정치를 사용한다. `A2C.train()` 메소드는 매 time step마다 호출된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class A2C:\n",
    "    def __init__(self,\n",
    "                 obs_features: int,\n",
    "                 num_actions: int,\n",
    "                 gamma: float = 0.99) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.actor = Actor(obs_features, num_actions)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "        \n",
    "        self.critic = Critic(obs_features)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "        \n",
    "    def select_action(self, obs: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns action and log pi(a|s).\n",
    "        \"\"\"\n",
    "        logits = self.actor(obs)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_pi = dist.log_prob(action)\n",
    "        return action, log_pi\n",
    "    \n",
    "    def train(self,\n",
    "              obs: torch.Tensor,\n",
    "              next_obs: torch.Tensor,\n",
    "              reward: torch.Tensor,\n",
    "              terminated: torch.Tensor,\n",
    "              log_pi: torch.Tensor) -> Tuple[float, float]:\n",
    "        state_value = self.critic(obs)\n",
    "        with torch.no_grad():\n",
    "            next_state_value = self.critic(next_obs)\n",
    "            \n",
    "        target_state_value = self._compute_target_state_value(next_state_value, reward, terminated)\n",
    "        advantage = self._compute_advantage(state_value.detach(), target_state_value)\n",
    "        \n",
    "        actor_loss = self._compute_actor_loss(log_pi, advantage)\n",
    "        critic_loss = self._compute_critic_loss(state_value, target_state_value)\n",
    "        \n",
    "        self._train_step(actor_loss, self.actor_optimizer)\n",
    "        self._train_step(critic_loss, self.critic_optimizer)\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "    \n",
    "    def _compute_target_state_value(self,\n",
    "                                   next_state_value: torch.Tensor,\n",
    "                                   reward: torch.Tensor,\n",
    "                                   terminated: torch.Tensor) -> torch.Tensor:\n",
    "        not_terminated = 1 - terminated\n",
    "        return reward + not_terminated * self.gamma * next_state_value\n",
    "    \n",
    "    def _compute_advantage(self,\n",
    "                          state_value: torch.Tensor,\n",
    "                          target_state_value: torch.Tensor) -> torch.Tensor:\n",
    "        return target_state_value - state_value\n",
    "    \n",
    "    def _compute_actor_loss(self,\n",
    "                           log_pi: torch.Tensor,\n",
    "                           advantage: torch.Tensor) -> torch.Tensor:\n",
    "        return -(log_pi * advantage).mean()\n",
    "    \n",
    "    def _compute_critic_loss(self,\n",
    "                            state_value: torch.Tensor,\n",
    "                            target_state_value: torch.Tensor) -> torch.Tensor:\n",
    "        return F.mse_loss(state_value, target_state_value)\n",
    "    \n",
    "    def _train_step(self,\n",
    "                    loss: torch.Tensor,\n",
    "                    optimizer: optim.Optimizer):\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make CartPole-v1 Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(env_id) # for training\n",
    "inference_env = gym.make(env_id, render_mode=\"human\") # for inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_features = env.observation_space.shape\n",
    "obs_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_actions = env.action_space.n\n",
    "num_actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = A2C(obs_features[0], num_actions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04262747,  0.04798423,  0.00884291, -0.00670878], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs, _ = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1), tensor(-0.5673, grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action, log_pi = agent.select_action(torch.from_numpy(obs))\n",
    "(action, log_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04358716,  0.24297826,  0.00870873, -0.2965886 ], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action.numpy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference - episode: 0, cumulative reward: 27.0\n",
      "inference - episode: 50, cumulative reward: 21.0\n",
      "inference - episode: 100, cumulative reward: 64.0\n",
      "inference - episode: 150, cumulative reward: 50.0\n",
      "inference - episode: 200, cumulative reward: 175.0\n",
      "inference - episode: 250, cumulative reward: 27.0\n",
      "inference - episode: 300, cumulative reward: 10.0\n",
      "inference - episode: 350, cumulative reward: 8.0\n",
      "inference - episode: 400, cumulative reward: 9.0\n",
      "inference - episode: 450, cumulative reward: 10.0\n",
      "inference - episode: 500, cumulative reward: 9.0\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "logger = SummaryWriter(\"results/CartPole-v1_A2C\")\n",
    "\n",
    "total_episodes = 501\n",
    "inference_freq = 50\n",
    "\n",
    "for episode in range(total_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    terminated = False\n",
    "    \n",
    "    cumulative_reward = 0\n",
    "    actor_loss_mean = 0.0\n",
    "    critic_loss_mean = 0.0\n",
    "    n = 0\n",
    "    \n",
    "    while not terminated:\n",
    "        # take action and observe\n",
    "        action, log_pi = agent.select_action(torch.from_numpy(obs))\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.numpy())\n",
    "        terminated = terminated | truncated\n",
    "        \n",
    "        # train the agent\n",
    "        actor_loss, critic_loss = agent.train(\n",
    "            torch.from_numpy(obs),\n",
    "            torch.from_numpy(next_obs),\n",
    "            torch.tensor(reward),\n",
    "            torch.tensor(terminated, dtype=torch.float32),\n",
    "            log_pi\n",
    "        )\n",
    "        \n",
    "        # update info\n",
    "        n += 1\n",
    "        actor_loss_mean += (actor_loss - actor_loss_mean) / n\n",
    "        critic_loss_mean += (critic_loss - critic_loss_mean) / n\n",
    "        cumulative_reward += reward\n",
    "        \n",
    "        # next step\n",
    "        obs = next_obs\n",
    "        \n",
    "    # log data\n",
    "    logger.add_scalar(\"Cumulative Reward\", cumulative_reward, episode)\n",
    "    logger.add_scalar(\"Actor Loss\", actor_loss_mean, episode)\n",
    "    logger.add_scalar(\"Critic Loss\", critic_loss_mean, episode)\n",
    "    \n",
    "    # inference\n",
    "    if episode % inference_freq == 0:\n",
    "        obs, _ = inference_env.reset()\n",
    "        terminated = False\n",
    "        inference_cumulative_reward = 0\n",
    "        while not terminated:\n",
    "            with torch.no_grad():\n",
    "                action, _ = agent.select_action(torch.from_numpy(obs))\n",
    "            next_obs, reward, terminated, truncated, _ = inference_env.step(action.numpy())\n",
    "            terminated = terminated | truncated\n",
    "\n",
    "            inference_cumulative_reward += reward\n",
    "            obs = next_obs\n",
    "            \n",
    "        print(f\"inference - episode: {episode}, cumulative reward: {inference_cumulative_reward}\")\n",
    "        \n",
    "logger.flush()\n",
    "logger.close()\n",
    "\n",
    "env.close()\n",
    "inference_env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-expert-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31ac01b8258adb5227428d8c0910a8335c058d339333bd5076ec56982dd3aaf7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
